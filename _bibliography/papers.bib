@inproceedings{cha-lee-2024-pre,
    title = "Pre-trained Language Models Return Distinguishable Probability Distributions to Unfaithfully Hallucinated Texts",
    author = "Cha, Taehun  and
      Lee, Donghun",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    website="https://aclanthology.org/2024.findings-emnlp.738/",
    pdf = "2024/EMNLP2024_Paper.pdf",
    slides = "2024/EMNLP2024_PPT.pdf",
    poster = "2024/EMNLP2024_Poster.pdf",
    doi = "10.18653/v1/2024.findings-emnlp.738",
    pages = "12630--12639",
    abstract = "In this work, we show the pre-trained language models return distinguishable generation probability and uncertainty distribution to unfaithfully hallucinated texts, regardless of their size and structure. By examining 24 models on 6 data sets, we find out that 88-98{\%} of cases return statistically significantly distinguishable generation probability and uncertainty distributions. Using this general phenomenon, we showcase a hallucination-reducing training algorithm. Our algorithm outperforms other baselines by achieving higher faithfulness metrics while maintaining sound general text quality measures.",
}

@inproceedings{cha-lee-2024-evaluating,
    title = "Evaluating Extrapolation Ability of Large Language Model in Chemical Domain",
    author = "Cha, Taehun  and
      Lee, Donghun",
    editor = "Edwards, Carl  and
      Wang, Qingyun  and
      Li, Manling  and
      Zhao, Lawrence  and
      Hope, Tom  and
      Ji, Heng",
    booktitle = "Proceedings of the 1st Workshop on Language + Molecules (L+M 2024)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    website = "https://aclanthology.org/2024.langmol-1.4",
    pdf = "2024/ACL2024_Workshop_Paper.pdf",
    poster = "2024/ACL2024_Workshop_Poster.pdf",
    doi = "10.18653/v1/2024.langmol-1.4",
    pages = "28--33",
    abstract = "Solving a problem outside the training space, i.e. extrapolation, has been a long problem in the machine learning community. The current success of large language models demonstrates the LLM{'}s extrapolation ability to several unseen tasks. In line with these works, we evaluate the LLM{''}s extrapolation ability in the chemical domain. We construct a data set measuring the material properties of epoxy polymers depending on various raw materials and curing processes. LLM should predict the material property when novel raw material is introduced utilizing its chemical knowledge. Through experiments, LLM tends to choose the right direction of adjustment but fails to determine the exact degree, resulting in poor MAE on some properties. But LLM can successfully adjust the degree with only a one-shot example. The results show that LLM can extrapolate to new unseen material utilizing its chemical knowledge learned through massive pre-training.",
}

@inproceedings{cha-lee-2024-sentencelda,
    title = "{S}entence{LDA}: Discriminative and Robust Document Representation with Sentence Level Topic Model",
    author = "Cha, Taehun  and
      Lee, Donghun",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    website = "https://aclanthology.org/2024.eacl-long.31",
    pdf = "2024/EACL2024_Paper.pdf",
    slides = "2024/EACL2024_PPT.pdf",
    pages = "521--538",
    abstract = "A subtle difference in context results in totally different nuances even for lexically identical words. On the other hand, two words can convey similar meanings given a homogeneous context. As a result, considering only word spelling information is not sufficient to obtain quality text representation. We propose SentenceLDA, a sentence-level topic model. We combine modern SentenceBERT and classical LDA to extend the semantic unit from word to sentence. By extending the semantic unit, we verify that SentenceLDA returns more discriminative document representation than other topic models, while maintaining LDA{'}s elegant probabilistic interpretability. We also verify the robustness of SentenceLDA by comparing the inference results on original and paraphrased texts. Additionally, we implement one possible application of SentenceLDA on corpus-level key opinion mining by applying SentenceLDA on an argumentative corpus, DebateSum.",
}

@inproceedings{cha-etal-2022-noun,
    title = "Noun-{MWP}: Math Word Problems Meet Noun Answers",
    author = "Cha, Taehun  and
      Jung, Jaeheun  and
      Lee, Donghun",
    editor = "Calzolari, Nicoletta  and
      Huang, Chu-Ren  and
      Kim, Hansaem  and
      Pustejovsky, James  and
      Wanner, Leo  and
      Choi, Key-Sun  and
      Ryu, Pum-Mo  and
      Chen, Hsin-Hsi  and
      Donatelli, Lucia  and
      Ji, Heng  and
      Kurohashi, Sadao  and
      Paggio, Patrizia  and
      Xue, Nianwen  and
      Kim, Seokhwan  and
      Hahm, Younggyun  and
      He, Zhong  and
      Lee, Tony Kyungil  and
      Santus, Enrico  and
      Bond, Francis  and
      Na, Seung-Hoon",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics",
    month = oct,
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    website = "https://aclanthology.org/2022.coling-1.338",
    pdf = "2022/COLING2022_Paper.pdf",
    slides = "2022/COLING2022_PPT.pdf",
    poster = "2022/COLING2022_Poster.pdf",
    pages = "3847--3857",
    abstract = "We introduce a new type of problems for math word problem (MWP) solvers, named Noun-MWPs, whose answer is a non-numerical string containing a noun from the problem text. We present a novel method to empower existing MWP solvers to handle Noun-MWPs, and apply the method on Expression-Pointer Transformer (EPT). Our model, N-EPT, solves Noun-MWPs significantly better than other models, and at the same time, solves conventional MWPs as well. Solving Noun-MWPs may lead to bridging MWP solvers and traditional question-answering NLP models.",
}
